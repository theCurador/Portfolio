{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "\n",
    "The World Baseball Classic (WBC) is an international baseball tournament organized by the International Baseball Federation (IBAF) and Major League Baseball (MLB). It was first held in 2006 and is currently held every four years. The tournament features national teams from around the world, including countries such as the United States, Japan, Cuba, and the Dominican Republic.\n",
    "\n",
    "In this project, we aim to gather information about the athletes that play in the World Baseball Classic and use it to measure the probabilities for the upcoming 2023 tournament. To do this, we will scrape data from Stathead.com, a website that provides baseball statistics, and analyze it using Python and JupyterLab. Our goal is to provide valuable insights into the players and teams participating in the WBC, which can be useful for baseball enthusiasts, analysts, and bettors alike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create a list of the World Baseball Classic (WBC) years we want to scrape data for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years of interest\n",
    "years = [2006, 2009, 2013, 2017, 2023]\n",
    "pools = ['A', 'B', 'C', 'D']\n",
    "rounds = ['Qualifying', 'Semifinals', 'Championship']\n",
    "predictions_answer = ['L', 'W']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from 06 WBC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qual_info_13 = pd.read_csv('WBC_data/2013_qualified_info.csv').assign(Year=years[2])\n",
    "quals_info_17 = pd.read_csv('WBC_data/2017_qualified_info.csv').assign(Year=years[3])\n",
    "quals_info_23 = pd.read_csv('WBC_data/2023_qualified_info.csv').assign(Year=years[4])\n",
    "\n",
    "# POOL COMPOSITION\n",
    "pool_comp_06 = pd.read_csv('WBC_data/2006_pool_composition.csv').assign(Year=years[0], Pools='')\n",
    "pool_comp_09 = pd.read_csv('WBC_data/2009_pool_composition.csv').assign(Year=years[1], Pools='')\n",
    "pool_comp_13 = pd.read_csv('WBC_data/2013_pool_composition.csv').assign(Year=years[2], Pools='')\n",
    "pool_comp_17 = pd.read_csv('WBC_data/2017_pool_composition.csv').assign(Year=years[3], Pools='')\n",
    "pool_comp_23 = pd.read_csv('WBC_data/2023_pool_composition.csv').assign(Year=years[4], Pools='')\n",
    "\n",
    "\n",
    "# POOL A,B,C,D RESULTS\n",
    "\n",
    "pool_ar_06 = pd.read_csv('WBC_data/2006_pool_a_results.csv').assign(Year=years[0], Pool='') \n",
    "pool_ar_09 = pd.read_csv('WBC_data/2009_pool_a_results.csv').assign(Year=years[1], Pool='') \n",
    "pool_ar_13 = pd.read_csv('WBC_data/2013_pool_a_results.csv').assign(Year=years[2], Pool='') \n",
    "pool_ar_17 = pd.read_csv('WBC_data/2017_pool_a_results.csv').assign(Year=years[3], Pool='')\n",
    "pool_ar_23 = pd.read_csv('WBC_data/2023_pool_a_results.csv').assign(Year=years[4], Pool='') \n",
    "\n",
    "pool_br_06 = pd.read_csv('WBC_data/2006_pool_b_results.csv').assign(Year=years[0], Pool='')\n",
    "pool_br_09 = pd.read_csv('WBC_data/2009_pool_b_results.csv').assign(Year=years[1], Pool='')\n",
    "pool_br_13 = pd.read_csv('WBC_data/2013_pool_b_results.csv').assign(Year=years[2], Pool='')\n",
    "pool_br_17 = pd.read_csv('WBC_data/2017_pool_b_results.csv').assign(Year=years[3], Pool='')\n",
    "pool_br_23 = pd.read_csv('WBC_data/2023_pool_b_results.csv').assign(Year=years[4], Pool='')\n",
    "\n",
    "pool_cr_06 = pd.read_csv('WBC_data/2006_pool_c_results.csv').assign(Year=years[0], Pool='')\n",
    "pool_cr_09 = pd.read_csv('WBC_data/2009_pool_c_results.csv').assign(Year=years[1], Pool='')\n",
    "pool_cr_13 = pd.read_csv('WBC_data/2013_pool_c_results.csv').assign(Year=years[2], Pool='')\n",
    "pool_cr_17 = pd.read_csv('WBC_data/2017_pool_c_results.csv').assign(Year=years[3], Pool='')\n",
    "pool_cr_23 = pd.read_csv('WBC_data/2023_pool_c_results.csv').assign(Year=years[4], Pool='')\n",
    "\n",
    "pool_dr_06 = pd.read_csv('WBC_data/2006_pool_d_results.csv').assign(Year=years[0], Pool='')\n",
    "pool_dr_09 = pd.read_csv('WBC_data/2009_pool_d_results.csv').assign(Year=years[1], Pool='')\n",
    "pool_dr_13 = pd.read_csv('WBC_data/2013_pool_d_results.csv').assign(Year=years[2], Pool='')\n",
    "pool_dr_17 = pd.read_csv('WBC_data/2017_pool_d_results.csv').assign(Year=years[3], Pool='')\n",
    "pool_dr_23 = pd.read_csv('WBC_data/2023_pool_d_results.csv').assign(Year=years[4], Pool='')\n",
    "\n",
    "#  SUMMARY\n",
    "pool_as_06 = pd.read_csv('WBC_data/2006_pool_a_summary.csv').assign(Year=years[0], Pool='')\n",
    "pool_as_13 = pd.read_csv('WBC_data/2013_pool_a_summary.csv').assign(Year=years[2])\n",
    "pool_as_17 = pd.read_csv('WBC_data/2017_pool_a_summary.csv').assign(Year=years[3]) \n",
    "pool_as_23 = pd.read_csv('WBC_data/2023_pool_a_summary.csv').assign(Year=years[4]) \n",
    "\n",
    "pool_bs_06 = pd.read_csv('WBC_data/2006_pool_b_summary.csv').assign(Year=years[0], Pool='')\n",
    "pool_bs_13 = pd.read_csv('WBC_data/2013_pool_b_summary.csv').assign(Year=years[2])\n",
    "pool_bs_17 = pd.read_csv('WBC_data/2017_pool_b_summary.csv').assign(Year=years[3])\n",
    "pool_bs_23 = pd.read_csv('WBC_data/2023_pool_b_summary.csv').assign(Year=years[4])\n",
    "\n",
    "\n",
    "pool_cs_06 = pd.read_csv('WBC_data/2006_pool_c_summary.csv').assign(Year=years[0], Pool='')\n",
    "pool_cs_13 = pd.read_csv('WBC_data/2013_pool_c_summary.csv').assign(Year=years[2])\n",
    "pool_cs_17 = pd.read_csv('WBC_data/2017_pool_c_summary.csv').assign(Year=years[3])\n",
    "pool_cs_23 = pd.read_csv('WBC_data/2023_pool_c_summary.csv').assign(Year=years[4])\n",
    "\n",
    "pool_ds_06 = pd.read_csv('WBC_data/2006_pool_d_summary.csv').assign(Year=years[0], Pool='')\n",
    "pool_ds_13 = pd.read_csv('WBC_data/2013_pool_d_summary.csv').assign(Year=years[2])\n",
    "pool_ds_17 = pd.read_csv('WBC_data/2017_pool_d_summary.csv').assign(Year=years[3])\n",
    "pool_ds_23 = pd.read_csv('WBC_data/2023_pool_d_summary.csv').assign(Year=years[4])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#POOL 1 & 2 RESULTS\n",
    "pool_1r_06 = pd.read_csv('WBC_data/2006_pool_1_results.csv').assign(Year=years[0])\n",
    "pool_1r_09 = pd.read_csv('WBC_data/2009_pool_1_results.csv').assign(Year=years[1])\n",
    "pool_1r_13 = pd.read_csv('WBC_data/2013_pool_1_results.csv').assign(Year=years[2])\n",
    "pool_1r_17 = pd.read_csv('WBC_data/2017_pool_1_results.csv').assign(Year=years[3])\n",
    "\n",
    "pool_2r_06 = pd.read_csv('WBC_data/2006_pool_2_results.csv').assign(Year=years[0])\n",
    "pool_2r_09 = pd.read_csv('WBC_data/2009_pool_2_results.csv').assign(Year=years[1])\n",
    "pool_2r_13 = pd.read_csv('WBC_data/2013_pool_2_results.csv').assign(Year=years[2])\n",
    "pool_2r_17 = pd.read_csv('WBC_data/2017_pool_2_results.csv').assign(Year=years[3])\n",
    "\n",
    "pool_1s_06 = pd.read_csv('WBC_data/2006_pool_1_summary.csv').assign(Year=years[0])\n",
    "pool_1s_17 = pd.read_csv('WBC_data/2017_pool_1_summary.csv').assign(Year=years[3])\n",
    "\n",
    "pool_2s_06 = pd.read_csv('WBC_data/2006_pool_2_summary.csv').assign(Year=years[0])\n",
    "pool_2s_17 = pd.read_csv('WBC_data/2017_pool_2_summary.csv').assign(Year=years[3])\n",
    "\n",
    "# INFORMATION TABLE\n",
    "quarterfinals_23 = pd.read_csv('WBC_data/2023_quarterfinals.csv').assign(Year=years[4])\n",
    "\n",
    "semifinals_09 = pd.read_csv('WBC_data/2009_semifinals.csv').assign(Year=years[1])\n",
    "semifinals_13 = pd.read_csv('WBC_data/2013_semifinals.csv').assign(Year=years[2])\n",
    "semifinals_17 = pd.read_csv('WBC_data/2017_semifinals.csv').assign(Year=years[3])\n",
    "semifinals_23 = pd.read_csv('WBC_data/2023_semifinals.csv').assign(Year=years[4])\n",
    "\n",
    "champ_round_06 = pd.read_csv('WBC_data/2006_champ_round.csv').assign(Year=years[0])\n",
    "champ_round_09 = pd.read_csv('WBC_data/2009_champ_round.csv').assign(Year=years[1])\n",
    "champ_round_13 = pd.read_csv('WBC_data/2013_champ_round.csv').assign(Year=years[2])\n",
    "champ_round_17 = pd.read_csv('WBC_data/2017_champ_round.csv').assign(Year=years[3])\n",
    "champ_round_23 = pd.read_csv('WBC_data/2023_champ_round.csv').assign(Year=years[4])\n",
    "\n",
    "\n",
    "\n",
    "wbc_classic_team_06 = pd.read_csv('WBC_data/2006_classic_team.csv').assign(Year=years[0])\n",
    "wbc_classic_team_09 = pd.read_csv('WBC_data/2009_classic_team.csv').assign(Year=years[1])\n",
    "wbc_classic_team_13 = pd.read_csv('WBC_data/2013_classic_team.csv').assign(Year=years[2])\n",
    "wbc_classic_team_17 = pd.read_csv('WBC_data/2017_classic_team.csv').assign(Year=years[3])\n",
    "\n",
    "final_standings_06 = pd.read_csv('WBC_data/2006_final_standing.csv').assign(Year=years[0])\n",
    "final_standings_09 = pd.read_csv('WBC_data/2009_final_standing.csv').assign(Year=years[1])\n",
    "final_standings_13 = pd.read_csv('WBC_data/2013_final_standing.csv').assign(Year=years[2])\n",
    "final_standings_17 = pd.read_csv('WBC_data/2017_final_standing.csv').assign(Year=years[3])\n",
    "\n",
    "\n",
    "batting_leaders_06 = pd.read_csv('WBC_data/2006_leader_batting.csv').assign(Year=years[0])\n",
    "batting_leaders_09 = pd.read_csv('WBC_data/2009_leader_batting.csv').assign(Year=years[1])\n",
    "batting_leaders_13 = pd.read_csv('WBC_data/2013_leader_batting.csv').assign(Year=years[2])\n",
    "batting_leaders_17 = pd.read_csv('WBC_data/2017_leader_batting.csv').assign(Year=years[3])\n",
    "\n",
    "pitching_leaders_06 = pd.read_csv('WBC_data/2006_leader_pitching.csv').assign(Year=years[0])\n",
    "pitching_leaders_09 = pd.read_csv('WBC_data/2009_leader_pitching.csv').assign(Year=years[1])\n",
    "pitching_leaders_13 = pd.read_csv('WBC_data/2013_leader_pitching.csv').assign(Year=years[2])\n",
    "pitching_leaders_17 = pd.read_csv('WBC_data/2017_leader_pitching.csv').assign(Year=years[3])\n",
    "\n",
    "wbc_champions_06 = pd.read_csv('WBC_data/2006_champ_info.csv').assign(Year=years[0])\n",
    "wbc_champions_09 = pd.read_csv('WBC_data/2009_champ_info.csv').assign(Year=years[1])\n",
    "wbc_champions_13 = pd.read_csv('WBC_data/2013_champ_info.csv').assign(Year=years[2])\n",
    "wbc_champions_17 = pd.read_csv('WBC_data/2017_champ_info.csv').assign(Year=years[3])\n",
    "\n",
    "# VENUES\n",
    "venues_a_06 = pd.read_csv('WBC_data/2006_venues_1.csv').assign(Year=years[0])\n",
    "venues_b_06 = pd.read_csv('WBC_data/2006_venues_2.csv').assign(Year=years[0])\n",
    "\n",
    "venues_a_09 = pd.read_csv('WBC_data/2009_venues_1.csv').assign(Year=years[1])\n",
    "venues_b_09 = pd.read_csv('WBC_data/2009_venues_2.csv').assign(Year=years[1])\n",
    "\n",
    "venues_a_13 = pd.read_csv('WBC_data/2013_venues_1.csv').assign(Year=years[2])\n",
    "venues_b_13 = pd.read_csv('WBC_data/2013_venues_2.csv').assign(Year=years[2])\n",
    "\n",
    "venues_a_17 = pd.read_csv('WBC_data/2017_venues_1.csv').assign(Year=years[3])\n",
    "venues_b_17 = pd.read_csv('WBC_data/2017_venues_2.csv').assign(Year=years[3])\n",
    "\n",
    "venues_a_23 = pd.read_csv('WBC_data/2023_venues_1.csv').assign(Year=years[4])\n",
    "\n",
    "# BASECAMP INFO\n",
    "base_camp_a_23 = pd.read_csv('WBC_data/2023_team_base_camp.csv').assign(Year=years[4])\n",
    "base_camp_b_23 = pd.read_csv('WBC_data/2023_team_base_camp_2.csv').assign(Year=years[4])\n",
    "\n",
    "\n",
    "# BROADCAST INFO\n",
    "broadcast_tv_17 = pd.read_csv('WBC_data/2017_broadcast_tv.csv').assign(Year=years[3])\n",
    "broadcast_radio_17 = pd.read_csv('WBC_data/2017_broadcast_radio.csv').assign(Year=years[3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 85)\n",
    "pd.set_option ('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melt Data for 2006 - 23\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Name' that holds the names in the Pool A-D columns\n",
    "#pool_comp_06['Name'] = pool_comp_06[['Pool A', 'Pool B', 'Pool C', 'Pool D']].apply(lambda x: ', '.join(x.dropna()), axis=1)\n",
    "\n",
    "#2006\n",
    "\n",
    "# Melt the dataframe to convert Pool A-D into a single 'Pool' column\n",
    "melted_df = pd.melt(pool_comp_06, id_vars=['Year'], value_vars=['Pool A', 'Pool B', 'Pool C', 'Pool D'], var_name='Pool')\n",
    "\n",
    "# Rename the value column to 'Team'\n",
    "melted_df = melted_df.rename(columns={'value': 'Team'})\n",
    "\n",
    "# Drop any rows with missing values\n",
    "melted_06_comp_df = melted_df.dropna()\n",
    "\n",
    "\n",
    "#2009\n",
    "\n",
    "# Melt the DataFrame to convert Pool's A-D into a single column\n",
    "melted_09_df = pd.melt(pool_comp_09, id_vars=['Year'], value_vars=['Pool A', 'Pool B', 'Pool C', 'Pool D'], var_name='Pool')\n",
    "\n",
    "# Rename the value a column to 'Team'\n",
    "melted_09_df = melted_09_df.rename(columns={'value':'Team'})\n",
    "\n",
    "# Drop any rows with missing values\n",
    "melted_09_comp_df = melted_09_df.dropna()\n",
    "\n",
    "#2013\n",
    "\n",
    "# Melt the DataFrame to convert Pool's A-D into a single column\n",
    "melted_13_df = pd.melt(pool_comp_13, id_vars=['Year'], value_vars=['Pool A', 'Pool B', 'Pool C', 'Pool D'], var_name='Pool')\n",
    "\n",
    "# Rename the value a column to 'Team'\n",
    "melted_13_df = melted_13_df.rename(columns={'value':'Team'})\n",
    "\n",
    "# Drop any rows with missing values\n",
    "melted_13_comp_df = melted_13_df.dropna()\n",
    "\n",
    "#2017\n",
    "\n",
    "# Melt the DataFrame to convert Pool's A-D into a single column\n",
    "melted_17_df = pd.melt(pool_comp_17, id_vars=['Year'], value_vars=['Pool A', 'Pool B', 'Pool C', 'Pool D'], var_name='Pool')\n",
    "\n",
    "# Rename the value a column to 'Team'\n",
    "melted_17_df = melted_17_df.rename(columns={'value':'Team'})\n",
    "\n",
    "# Drop any rows with missing values\n",
    "melted_17_comp_df = melted_17_df.dropna()\n",
    "\n",
    "#2023\n",
    "\n",
    "# Melt the DataFrame to convert Pool's A-D into a single column\n",
    "melted_23_df = pd.melt(pool_comp_23, id_vars=['Year'], value_vars=['Pool A', 'Pool B', 'Pool C', 'Pool D'], var_name='Pool')\n",
    "\n",
    "# Rename the value a column to 'Team'\n",
    "melted_23_df = melted_23_df.rename(columns={'value':'Team'})\n",
    "\n",
    "# Drop any rows with missing values\n",
    "melted_23_comp_df = melted_23_df.dropna()\n",
    "\n",
    "# melted_06_comp_df\n",
    "# melted_09_comp_df\n",
    "# melted_13_comp_df\n",
    "# melted_17_comp_df\n",
    "# melted_23_comp_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_comp_df = pd.concat([melted_06_comp_df, \n",
    "                          melted_09_comp_df,\n",
    "                          melted_13_comp_df,\n",
    "                          melted_17_comp_df,\n",
    "                          melted_23_comp_df[['Team', 'Pool']]], \n",
    "                         axis=0, \n",
    "                         ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  China\n",
       "1         Chinese Taipei\n",
       "2                  Japan\n",
       "3            South Korea\n",
       "4                 Canada\n",
       "5                 Mexico\n",
       "6           South Africa\n",
       "7          United States\n",
       "8                   Cuba\n",
       "9            Netherlands\n",
       "10                Panama\n",
       "11           Puerto Rico\n",
       "12             Australia\n",
       "13    Dominican Republic\n",
       "14                 Italy\n",
       "15             Venezuela\n",
       "Name: Team, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted_06_comp_df['Team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Pool A\n",
       "1     Pool A\n",
       "2     Pool A\n",
       "3     Pool A\n",
       "4     Pool B\n",
       "5     Pool B\n",
       "6     Pool B\n",
       "7     Pool B\n",
       "8     Pool C\n",
       "9     Pool C\n",
       "10    Pool C\n",
       "11    Pool C\n",
       "12    Pool D\n",
       "13    Pool D\n",
       "14    Pool D\n",
       "15    Pool D\n",
       "Name: Pool, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted_09_comp_df.Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POOL A SUMMARY\n",
    "#have to remove the (h) from japan in order to match with the previous melted. \n",
    "\n",
    "#if you run pool_as_06 you will notice Japan has 'Japan (H)' and is not recognized by the other table\n",
    "\n",
    "#I removed the H with the following code\n",
    "pool_as_06['Team'] = pool_as_06['Team'].str.replace(' \\(H\\)', '', regex=True)\n",
    "\n",
    "#Now I wanted to merge this witht he melted comp that contains year, pool and team names ..\n",
    "merged_as_06 = pd.merge(pool_as_06, melted_06_comp_df[['Team', 'Pool']], on='Team', how='left')\n",
    "\n",
    "#after priting noticed it had two pool Pool X and Pool Y\n",
    "#so I decided to drop Pool X as which was empty \n",
    "merged_as_06 = merged_as_06.drop(labels='Pool_x', axis=1)\n",
    "\n",
    "#after that decided to rename() from Pool Y to Pool \n",
    "merged_as_06.rename(columns={'Pool_y': 'Pool'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# POOL B SUMMARY\n",
    "\n",
    "#need to follow the same protocol and remove the H out of United States /.. \n",
    "# if not it will say[2 United States (H) 3 2 1 25 8 +17 etc...] \n",
    "pool_bs_06['Team'] = pool_bs_06['Team'].str.replace(' \\(H\\)', '', regex=True)\n",
    "\n",
    "#Now we print to test\n",
    "merge_bs_06 = pd.merge(pool_bs_06, melted_06_comp_df[['Team', 'Pool']], on='Team', how='left')\n",
    "\n",
    "#now the problem of fixin the pool drop() and rename() is a good practice\n",
    "merged_bs_06 = merge_bs_06.drop(labels='Pool_x', axis=1)\n",
    "merged_bs_06.rename(columns={'Pool_y': 'Pool'}, inplace=True)\n",
    "\n",
    "\n",
    "# POOL C SUMMARY \n",
    "\n",
    "#need to follow the same protocol and remove the H out of United States /.. \n",
    "# if not it will say[2 United States (H) 3 2 1 25 8 +17 etc...] \n",
    "pool_cs_06['Team'] = pool_cs_06['Team'].str.replace(' \\(H\\)', '', regex=True)\n",
    "\n",
    "#Now we print to test\n",
    "merge_cs_06 = pd.merge(pool_cs_06, melted_06_comp_df[['Team', 'Pool']], on='Team', how='left')\n",
    "\n",
    "#now the problem of fixin the pool drop() and rename() is a good practice\n",
    "merged_cs_06 = merge_cs_06.drop(labels='Pool_x', axis=1)\n",
    "merged_cs_06.rename(columns={'Pool_y': 'Pool'}, inplace=True)\n",
    "\n",
    "# POOL D SUMMARY \n",
    "\n",
    "#need to follow the same protocol and remove the H out of United States /.. \n",
    "# if not it will say[2 United States (H) 3 2 1 25 8 +17 etc...] \n",
    "pool_ds_06['Team'] = pool_ds_06['Team'].str.replace(' \\(H\\)', '', regex=True)\n",
    "\n",
    "#Now we print to test\n",
    "merge_ds_06 = pd.merge(pool_ds_06, melted_06_comp_df[['Team', 'Pool']], on='Team', how='left')\n",
    "\n",
    "#now the problem of fixin the pool drop() and rename() is a good practice\n",
    "merged_ds_06 = merge_ds_06.drop(labels='Pool_x', axis=1)\n",
    "merged_ds_06.rename(columns={'Pool_y': 'Pool'}, inplace=True)\n",
    "\n",
    "# Semi Finals\n",
    "\n",
    "# POOL 1 SUMMARY \n",
    "\n",
    "new_1s_06 = pd.merge(pool_1s_06, melted_06_comp_df, on='Team')\n",
    "new_1s_06 = new_1s_06.drop(labels='Year_x', axis=1)\n",
    "new_1s_06.rename(columns={'Year_Y': 'Year'}, inplace=True)\n",
    "\n",
    "# POOL 2 SUMMARY\n",
    "new_2s_06 = pd.merge(pool_2s_06, melted_06_comp_df, on='Team')\n",
    "new_2s_06 = new_2s_06.drop(labels='Year_x', axis=1)\n",
    "new_2s_06.rename(columns={'Year_Y': 'Year'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thinking i might have to do this with the results as well. \n",
    "merged_ps1_06 = pd.merge(merged_as_06, merged_bs_06[['Team', 'Pool', 'Qualification']], on='Team', how='left')\n",
    "merged_ps1_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print A Summary\n",
    "merged_bs_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print B Summary\n",
    "\n",
    "merged_bs_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print C Summary\n",
    "merged_cs_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print D Summary\n",
    "merged_ds_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to follow the same protocol and remove the H out of United States /.. \n",
    "# if not it will say[2 United States (H) 3 2 1 25 8 +17 etc...] \n",
    "merged_ds_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_2s_06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2009\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2013\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
