{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b60afd-5ccf-4402-94f2-37a50dba42b9",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "Using a more sophisticated machine learning algorithm. \n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "Decision tree leaves us with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data. \n",
    "\n",
    "Even today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the **random forest** as an example.\n",
    "\n",
    "The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy tahn a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters. \n",
    "\n",
    "**Example**\n",
    "\n",
    "We've already seen the code to load the data a few times. At the end of data-loading, we have the following variables:\n",
    "\n",
    "- train_X\n",
    "- val_X\n",
    "- train_y\n",
    "- val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d198cd-39e2-4bcc-a6a6-d36f820f4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Load data \n",
    "\n",
    "\n",
    "melbourne_path = '/Users/jeanzayas/Desktop/Divergence/DATA ANALYSIS/Learning/ML kaggle learn/Data_ML_try/Melbourne Housing/melb_data.csv'\n",
    "\n",
    "melbourne_data = pd.read_csv(melbourne_path)\n",
    "\n",
    "# Filter rows with missing values\n",
    "melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "# Choose target and features \n",
    "y = melbourne_data.Price\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "\n",
    "X = melbourne_data[melbourne_features]\n",
    "\n",
    "\n",
    "#--------------------\n",
    "# Split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator.\n",
    "# Supplying a numeric value to the random_state argument guarantees we get the split every time we run this script. \n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b85c901-9dea-44a1-8a9b-1161e71e7e51",
   "metadata": {},
   "source": [
    "We build a random forest model similarly to how we built a decision tree in scikit-learn -this time using the {RandomForestRegressor} class instead of DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c175798-bb81-4b88-a391-5cb5a053f8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE is:  191669.7536453626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y) \n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print('The MAE is: ', mean_absolute_error(val_y, melb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0eda5-dc7f-4629-a015-82c9537a6b4f",
   "metadata": {},
   "source": [
    "There is likely room for further improvement, but this is a big improvement over the best decision tree error of 250,000. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33990740-6a4d-4e07-bfaa-578f251ecb36",
   "metadata": {},
   "source": [
    "# _________________\n",
    "\n",
    "## Recap\n",
    " Here's the code we've written so far. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8674a6d-58dc-47c6-ad6e-7ad753b7d6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE when not specifying max_leaf_nodes: 29,653\n",
      "Validation MAE for best value of max_leaf_nodes: 27,283\n"
     ]
    }
   ],
   "source": [
    "# Path of the file to read\n",
    "\n",
    "iowa_file_path = '/Users/jeanzayas/Desktop/Divergence/DATA ANALYSIS/Learning/ML kaggle learn/Data_ML_try/Housing Prices/train.csv'\n",
    "\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "\n",
    "# Create target object and call it y\n",
    "y = home_data.SalePrice\n",
    "\n",
    "# Create X\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = home_data[features]\n",
    "\n",
    "# Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Specify Model\n",
    "iowa_model = DecisionTreeRegressor(random_state=1)\n",
    "# Fit Model\n",
    "iowa_model.fit(train_X, train_y)\n",
    "\n",
    "# Make validation predictions and calculate mean absolute error\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
    "\n",
    "# Using best value for max_leaf_nodes\n",
    "iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n",
    "iowa_model.fit(train_X, train_y)\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12c668-1d0b-455a-80f8-d87ed1d32131",
   "metadata": {},
   "source": [
    "Data science isn't always this easy. But replacing the decision tree with a Random Forest is going to be an easy win.\n",
    "\n",
    "## Use a Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0ebacc-7ba8-4a3b-9448-d0d44b65bbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE for Random Forest Model: 21857.15912981083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the model. Set random_state to 1\n",
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# fit your model\n",
    "rf_model.fit(train_X, train_y)\n",
    "# Calculate the mean absolute error of your Random Forest model on the validation data\n",
    "rf_val_prediction = rf_model.predict(val_X)\n",
    "rf_val_mae = mean_absolute_error(val_y, rf_val_prediction)\n",
    "\n",
    "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82ea6f-5786-4483-b184-9ebb983a747f",
   "metadata": {},
   "source": [
    "# Train a model for the competition \n",
    "\n",
    "The code cell aboce trains a Random Forest model on \"train_X\" and \"train_y\",\n",
    "Use the code cell below to build a Random FOrest model and train it on all of \"X\" and \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60f282d-c75d-4f0f-b377-851ac6993b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve accuracy, create a new Random Forest model which you will train on all training data\n",
    "rf_model_on_full_data = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# fit rf_model_on_full_data on all data from the training data\n",
    "rf_model_on_full_data.fit(X, y)\n",
    "\n",
    "\n",
    "# path to file you will use for predictions\n",
    "test_data_path = '/Users/jeanzayas/Desktop/Divergence/DATA ANALYSIS/Learning/ML kaggle learn/Data_ML_try/Housing Prices/test.csv'\n",
    "\n",
    "# read test data file using pandas\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# create test_X which comes from test_data but includes only the columns you used for prediction.\n",
    "# The list of columns is stored in a variable called features\n",
    "\n",
    "test_X = test_data[features]\n",
    "\n",
    "# make predictions which we will submit. \n",
    "test_preds = rf_model_on_full_data.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d6c99-4dcc-4057-9db8-cb5c94dc5d7c",
   "metadata": {},
   "source": [
    "### Generate a submission\n",
    "\n",
    "Run the code cell below to generate a CSV file with your predictions that you can use to submit to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1755e975-4044-40d5-a319-48013bf206cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code to save predictions in the format used for competition scoring\n",
    "\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': test_preds})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008729d-67f8-4c4f-b9cd-0074588bb16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
